{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.layers import *\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from art.attacks import FastGradientMethod\n",
    "from art.classifiers import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if True:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.5075 - accuracy: 0.9138\n",
      "Test loss: 0.5075193762779235\n",
      "Test accuracy: 0.9138\n"
     ]
    }
   ],
   "source": [
    "model = load_model('Resnet18-200epochs.h5')\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model, clip_values=None, use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM Samples creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm1 = FastGradientMethod(classifier, eps=0.01)\n",
    "train1 = fgsm1.generate(x_train)\n",
    "test1 = fgsm1.generate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm2 = FastGradientMethod(classifier, eps=0.1)\n",
    "train2 = fgsm2.generate(x_train)\n",
    "test2 = fgsm2.generate(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "save('fgsmtrain(0.01).npy', train1)\n",
    "save('fgsmtest(0.01).npy', test1)\n",
    "save('fgsmtrain(0.1).npy', train2)\n",
    "save('fgsmtest(0.1).npy', test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fgsm 0.1 test examples: 24.97%\n"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "data = load('fgsmtest(0.01).npy')\n",
    "predictions = model.predict(data)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on fgsm 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
    "x_train_noisy = x_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
    "x_test_noisy = x_test + noise\n",
    "\n",
    "train3 = np.clip(x_train_noisy, 0., 1.)\n",
    "test3 = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 4\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test1[idx])\n",
    "plt.title('0.01')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(test2[idx])\n",
    "plt.title('0.1')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test3[idx])\n",
    "plt.title('noisy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model Accuracy on FGSM Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fgsm 0.01 test examples: 24.97%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test1)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on fgsm 0.01 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fgsm 0.1 test examples: 9.58%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test2)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on fgsm 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE trained on eps = 0.01 fgsm samples solely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0375\n",
      "Epoch 00001: val_loss improved from inf to 0.03571, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 106s 2ms/sample - loss: 0.0375 - val_loss: 0.0357\n",
      "Epoch 2/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0301\n",
      "Epoch 00002: val_loss improved from 0.03571 to 0.02929, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0301 - val_loss: 0.0293\n",
      "Epoch 3/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00003: val_loss improved from 0.02929 to 0.02911, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0291\n",
      "Epoch 4/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00004: val_loss improved from 0.02911 to 0.02883, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0297 - val_loss: 0.0288\n",
      "Epoch 5/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00005: val_loss improved from 0.02883 to 0.02882, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0296 - val_loss: 0.0288\n",
      "Epoch 6/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0295\n",
      "Epoch 00006: val_loss improved from 0.02882 to 0.02869, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0295 - val_loss: 0.0287\n",
      "Epoch 7/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0295\n",
      "Epoch 00007: val_loss improved from 0.02869 to 0.02867, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0295 - val_loss: 0.0287\n",
      "Epoch 8/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0295\n",
      "Epoch 00008: val_loss did not improve from 0.02867\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0295 - val_loss: 0.0287\n",
      "Epoch 9/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00009: val_loss improved from 0.02867 to 0.02862, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 10/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00010: val_loss did not improve from 0.02862\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0294 - val_loss: 0.0289\n",
      "Epoch 11/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00011: val_loss did not improve from 0.02862\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0294 - val_loss: 0.0288\n",
      "Epoch 12/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00012: val_loss improved from 0.02862 to 0.02856, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 13/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00013: val_loss did not improve from 0.02856\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 14/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00014: val_loss did not improve from 0.02856\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0293 - val_loss: 0.0286\n",
      "Epoch 15/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00015: val_loss improved from 0.02856 to 0.02852, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0294 - val_loss: 0.0285\n",
      "Epoch 16/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00016: val_loss improved from 0.02852 to 0.02850, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 17/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00017: val_loss improved from 0.02850 to 0.02848, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 18/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00018: val_loss did not improve from 0.02848\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 19/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00019: val_loss improved from 0.02848 to 0.02847, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 20/20\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00020: val_loss improved from 0.02847 to 0.02847, saving model to best_model1.h5\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f491157ffd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae1 = denoising_autoencoder()\n",
    "dae1.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "dae1.fit(train1,\n",
    "       train1,\n",
    "       validation_data=(test1, test1),\n",
    "       epochs=20,\n",
    "       batch_size=128,\n",
    "       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on denoised 0.01 test examples: 38.92%\n",
      "Accuracy on Original test examples: 53.56999999999999%\n",
      "Accuracy on denoised 0.1 test examples: 12.3%\n"
     ]
    }
   ],
   "source": [
    "test11 = dae1.predict(test1)\n",
    "test12 = dae1.predict(x_test)\n",
    "test13 = dae1.predict(test2)\n",
    "\n",
    "pred1 = model.predict(test11)\n",
    "pred2 = model.predict(test12)\n",
    "pred3 = model.predict(test13)\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred1, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.01 test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred2, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on Original test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred3, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE trained on eps = 0.1 fgsm samples solely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0428\n",
      "Epoch 00001: val_loss improved from inf to 0.04033, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 0.0428 - val_loss: 0.0403\n",
      "Epoch 2/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0364\n",
      "Epoch 00002: val_loss improved from 0.04033 to 0.03521, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0364 - val_loss: 0.0352\n",
      "Epoch 3/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0358\n",
      "Epoch 00003: val_loss improved from 0.03521 to 0.03485, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0358 - val_loss: 0.0348\n",
      "Epoch 4/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0355\n",
      "Epoch 00004: val_loss improved from 0.03485 to 0.03455, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0355 - val_loss: 0.0346\n",
      "Epoch 5/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0353\n",
      "Epoch 00005: val_loss improved from 0.03455 to 0.03434, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0353 - val_loss: 0.0343\n",
      "Epoch 6/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0352\n",
      "Epoch 00006: val_loss did not improve from 0.03434\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0352 - val_loss: 0.0347\n",
      "Epoch 7/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0351\n",
      "Epoch 00007: val_loss did not improve from 0.03434\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0351 - val_loss: 0.0345\n",
      "Epoch 8/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0350\n",
      "Epoch 00008: val_loss improved from 0.03434 to 0.03406, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0350 - val_loss: 0.0341\n",
      "Epoch 9/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0349\n",
      "Epoch 00009: val_loss did not improve from 0.03406\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0349 - val_loss: 0.0341\n",
      "Epoch 10/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0349\n",
      "Epoch 00010: val_loss did not improve from 0.03406\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0349 - val_loss: 0.0342\n",
      "Epoch 11/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0348\n",
      "Epoch 00011: val_loss improved from 0.03406 to 0.03391, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0348 - val_loss: 0.0339\n",
      "Epoch 12/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0348\n",
      "Epoch 00012: val_loss improved from 0.03391 to 0.03387, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0348 - val_loss: 0.0339\n",
      "Epoch 13/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0347\n",
      "Epoch 00013: val_loss improved from 0.03387 to 0.03380, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0347 - val_loss: 0.0338\n",
      "Epoch 14/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0347\n",
      "Epoch 00014: val_loss did not improve from 0.03380\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0347 - val_loss: 0.0340\n",
      "Epoch 15/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0346\n",
      "Epoch 00015: val_loss improved from 0.03380 to 0.03372, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0346 - val_loss: 0.0337\n",
      "Epoch 16/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0346\n",
      "Epoch 00016: val_loss improved from 0.03372 to 0.03367, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0346 - val_loss: 0.0337\n",
      "Epoch 17/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0346\n",
      "Epoch 00017: val_loss improved from 0.03367 to 0.03364, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0346 - val_loss: 0.0336\n",
      "Epoch 18/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00018: val_loss did not improve from 0.03364\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0345 - val_loss: 0.0337\n",
      "Epoch 19/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00019: val_loss improved from 0.03364 to 0.03360, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0345 - val_loss: 0.0336\n",
      "Epoch 20/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00020: val_loss did not improve from 0.03360\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0345 - val_loss: 0.0338\n",
      "Epoch 21/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00021: val_loss did not improve from 0.03360\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0345 - val_loss: 0.0336\n",
      "Epoch 22/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0345\n",
      "Epoch 00022: val_loss improved from 0.03360 to 0.03355, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0345 - val_loss: 0.0336\n",
      "Epoch 23/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00023: val_loss did not improve from 0.03355\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0336\n",
      "Epoch 24/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00024: val_loss did not improve from 0.03355\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0336\n",
      "Epoch 25/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00025: val_loss did not improve from 0.03355\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0338\n",
      "Epoch 26/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00026: val_loss improved from 0.03355 to 0.03355, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0335\n",
      "Epoch 27/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00027: val_loss improved from 0.03355 to 0.03351, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 0.0344 - val_loss: 0.0335\n",
      "Epoch 28/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00028: val_loss did not improve from 0.03351\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 0.0344 - val_loss: 0.0335\n",
      "Epoch 29/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00029: val_loss did not improve from 0.03351\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0335\n",
      "Epoch 30/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00030: val_loss did not improve from 0.03351\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0336\n",
      "Epoch 31/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00031: val_loss did not improve from 0.03351\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0344 - val_loss: 0.0336\n",
      "Epoch 32/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00032: val_loss did not improve from 0.03351\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0343 - val_loss: 0.0336\n",
      "Epoch 33/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00033: val_loss improved from 0.03351 to 0.03350, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00034: val_loss did not improve from 0.03350\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 35/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00035: val_loss improved from 0.03350 to 0.03348, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 36/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00036: val_loss improved from 0.03348 to 0.03347, saving model to best_model2.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 37/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00037: val_loss did not improve from 0.03347\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 38/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00038: val_loss did not improve from 0.03347\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 39/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00039: val_loss did not improve from 0.03347\n",
      "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n",
      "Epoch 40/40\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00040: val_loss did not improve from 0.03347\n",
      "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.0343 - val_loss: 0.0335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f494445bed0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae2 = denoising_autoencoder()\n",
    "dae2.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model2.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "dae2.fit(train2,\n",
    "       train2,\n",
    "       validation_data=(test2, test2),\n",
    "       epochs=40,\n",
    "       batch_size=128,\n",
    "       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on denoised 0.01 test examples: 35.260000000000005%\n",
      "Accuracy on Original test examples: 54.92%\n",
      "Accuracy on denoised 0.1 test examples: 10.209999999999999%\n"
     ]
    }
   ],
   "source": [
    "test11 = dae2.predict(test1)\n",
    "test12 = dae2.predict(x_test)\n",
    "test13 = dae2.predict(test2)\n",
    "\n",
    "pred1 = model.predict(test11)\n",
    "pred2 = model.predict(test12)\n",
    "pred3 = model.predict(test13)\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred1, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.01 test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred2, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on Original test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred3, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE trained on eps = 0.01 fgsm samples and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0351\n",
      "Epoch 00001: val_loss improved from inf to 0.03377, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 107s 2ms/sample - loss: 0.0351 - val_loss: 0.0338\n",
      "Epoch 2/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0300\n",
      "Epoch 00002: val_loss improved from 0.03377 to 0.02925, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0300 - val_loss: 0.0292\n",
      "Epoch 3/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00003: val_loss improved from 0.02925 to 0.02882, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0297 - val_loss: 0.0288\n",
      "Epoch 4/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00004: val_loss improved from 0.02882 to 0.02874, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0296 - val_loss: 0.0287\n",
      "Epoch 5/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00005: val_loss improved from 0.02874 to 0.02866, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0296 - val_loss: 0.0287\n",
      "Epoch 6/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0295\n",
      "Epoch 00006: val_loss improved from 0.02866 to 0.02863, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0295 - val_loss: 0.0286\n",
      "Epoch 7/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00007: val_loss improved from 0.02863 to 0.02859, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 8/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00008: val_loss improved from 0.02859 to 0.02857, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 9/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00009: val_loss did not improve from 0.02857\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0294 - val_loss: 0.0286\n",
      "Epoch 10/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0294\n",
      "Epoch 00010: val_loss improved from 0.02857 to 0.02854, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0294 - val_loss: 0.0285\n",
      "Epoch 11/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00011: val_loss improved from 0.02854 to 0.02851, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 12/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00012: val_loss did not improve from 0.02851\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0293 - val_loss: 0.0286\n",
      "Epoch 13/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00013: val_loss improved from 0.02851 to 0.02849, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 14/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00014: val_loss did not improve from 0.02849\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0293 - val_loss: 0.0286\n",
      "Epoch 15/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00015: val_loss did not improve from 0.02849\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0293 - val_loss: 0.0285\n",
      "Epoch 16/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0293\n",
      "Epoch 00016: val_loss did not improve from 0.02849\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0292 - val_loss: 0.0288\n",
      "Epoch 17/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0292\n",
      "Epoch 00017: val_loss improved from 0.02849 to 0.02843, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0292 - val_loss: 0.0284\n",
      "Epoch 18/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0292\n",
      "Epoch 00018: val_loss improved from 0.02843 to 0.02842, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0292 - val_loss: 0.0284\n",
      "Epoch 19/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0292\n",
      "Epoch 00019: val_loss improved from 0.02842 to 0.02841, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0292 - val_loss: 0.0284\n",
      "Epoch 20/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0292\n",
      "Epoch 00020: val_loss improved from 0.02841 to 0.02840, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0292 - val_loss: 0.0284\n",
      "Epoch 21/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00021: val_loss improved from 0.02840 to 0.02838, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 22/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00022: val_loss improved from 0.02838 to 0.02838, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 23/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00023: val_loss did not improve from 0.02838\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 24/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00024: val_loss improved from 0.02838 to 0.02837, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 25/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00025: val_loss improved from 0.02837 to 0.02837, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 26/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00026: val_loss improved from 0.02837 to 0.02836, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 27/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00027: val_loss did not improve from 0.02836\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 28/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00028: val_loss did not improve from 0.02836\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0291 - val_loss: 0.0284\n",
      "Epoch 29/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00029: val_loss did not improve from 0.02836\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.0291 - val_loss: 0.0285\n",
      "Epoch 30/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00030: val_loss improved from 0.02836 to 0.02835, saving model to best_model4.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0291 - val_loss: 0.0283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f491126bc10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae4 = denoising_autoencoder()\n",
    "dae4.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model4.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "dae3.fit(train1,\n",
    "       x_train,\n",
    "       validation_data=(test1, x_test),\n",
    "       epochs=30,\n",
    "       batch_size=128,\n",
    "       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on denoised 0.01 test examples: 10.7%\n",
      "Accuracy on Original test examples: 10.73%\n",
      "Accuracy on denoised 0.1 test examples: 10.8%\n"
     ]
    }
   ],
   "source": [
    "test11 = dae4.predict(test1)\n",
    "test12 = dae4.predict(x_test)\n",
    "test13 = dae4.predict(test2)\n",
    "\n",
    "pred1 = model.predict(test11)\n",
    "pred2 = model.predict(test12)\n",
    "pred3 = model.predict(test13)\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred1, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.01 test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred2, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on Original test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred3, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE trained on eps = 0.1 fgsm samples and clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0362\n",
      "Epoch 00001: val_loss improved from inf to 0.03114, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 106s 2ms/sample - loss: 0.0361 - val_loss: 0.0311\n",
      "Epoch 2/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0306\n",
      "Epoch 00002: val_loss improved from 0.03114 to 0.03061, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0306 - val_loss: 0.0306\n",
      "Epoch 3/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0303\n",
      "Epoch 00003: val_loss improved from 0.03061 to 0.02940, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0303 - val_loss: 0.0294\n",
      "Epoch 4/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0302\n",
      "Epoch 00004: val_loss improved from 0.02940 to 0.02933, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0302 - val_loss: 0.0293\n",
      "Epoch 5/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0301\n",
      "Epoch 00005: val_loss improved from 0.02933 to 0.02932, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0301 - val_loss: 0.0293\n",
      "Epoch 6/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0300\n",
      "Epoch 00006: val_loss improved from 0.02932 to 0.02928, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0300 - val_loss: 0.0293\n",
      "Epoch 7/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0300\n",
      "Epoch 00007: val_loss improved from 0.02928 to 0.02919, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0300 - val_loss: 0.0292\n",
      "Epoch 8/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0300\n",
      "Epoch 00008: val_loss improved from 0.02919 to 0.02914, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0300 - val_loss: 0.0291\n",
      "Epoch 9/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0299\n",
      "Epoch 00009: val_loss did not improve from 0.02914\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0299 - val_loss: 0.0292\n",
      "Epoch 10/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0299\n",
      "Epoch 00010: val_loss improved from 0.02914 to 0.02908, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0299 - val_loss: 0.0291\n",
      "Epoch 11/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0299\n",
      "Epoch 00011: val_loss did not improve from 0.02908\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.0299 - val_loss: 0.0292\n",
      "Epoch 12/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00012: val_loss did not improve from 0.02908\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0291\n",
      "Epoch 13/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00013: val_loss improved from 0.02908 to 0.02906, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0291\n",
      "Epoch 14/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00014: val_loss improved from 0.02906 to 0.02902, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0290\n",
      "Epoch 15/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00015: val_loss did not improve from 0.02902\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0298 - val_loss: 0.0290\n",
      "Epoch 16/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00016: val_loss did not improve from 0.02902\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0292\n",
      "Epoch 17/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00017: val_loss did not improve from 0.02902\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.0298 - val_loss: 0.0291\n",
      "Epoch 18/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00018: val_loss improved from 0.02902 to 0.02900, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 100s 2ms/sample - loss: 0.0298 - val_loss: 0.0290\n",
      "Epoch 19/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00019: val_loss did not improve from 0.02900\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0298 - val_loss: 0.0290\n",
      "Epoch 20/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00020: val_loss did not improve from 0.02900\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0297 - val_loss: 0.0291\n",
      "Epoch 21/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00021: val_loss improved from 0.02900 to 0.02896, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 22/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00022: val_loss improved from 0.02896 to 0.02896, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 23/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00023: val_loss did not improve from 0.02896\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 24/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00024: val_loss did not improve from 0.02896\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 0.0297 - val_loss: 0.0290\n",
      "Epoch 25/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00025: val_loss improved from 0.02896 to 0.02894, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 0.0296 - val_loss: 0.0289\n",
      "Epoch 26/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00026: val_loss did not improve from 0.02894\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 0.0296 - val_loss: 0.0290\n",
      "Epoch 27/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00027: val_loss improved from 0.02894 to 0.02893, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 0.0296 - val_loss: 0.0289\n",
      "Epoch 28/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00028: val_loss did not improve from 0.02893\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 0.0296 - val_loss: 0.0290\n",
      "Epoch 29/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00029: val_loss improved from 0.02893 to 0.02892, saving model to best_model5.h5\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 0.0296 - val_loss: 0.0289\n",
      "Epoch 30/30\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 00030: val_loss did not improve from 0.02892\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 0.0296 - val_loss: 0.0289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4910eb8450>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae5 = denoising_autoencoder()\n",
    "dae5.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model5.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "dae5.fit(train2,\n",
    "       x_train,\n",
    "       validation_data=(test2, x_test),\n",
    "       epochs=30,\n",
    "       batch_size=128,\n",
    "       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on denoised 0.01 test examples: 45.26%\n",
      "Accuracy on Original test examples: 46.57%\n",
      "Accuracy on denoised 0.1 test examples: 31.430000000000003%\n"
     ]
    }
   ],
   "source": [
    "test11 = dae5.predict(test1)\n",
    "test12 = dae5.predict(x_test)\n",
    "test13 = dae5.predict(test2)\n",
    "\n",
    "pred1 = model.predict(test11)\n",
    "pred2 = model.predict(test12)\n",
    "pred3 = model.predict(test13)\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred1, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.01 test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred2, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on Original test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "accuracy = np.sum(np.argmax(pred3, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on denoised 0.1 test examples: {}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow1.1] *",
   "language": "python",
   "name": "conda-env-tensorflow1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
